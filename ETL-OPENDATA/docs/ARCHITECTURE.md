# ğŸ§± Architecture â€“ ETL Open Data

## ğŸ¯ Objectif

Construire un pipeline modulaire, traÃ§able et facilement rÃ©utilisable pour lâ€™intÃ©gration de jeux de donnÃ©es publics hÃ©tÃ©rogÃ¨nes vers DuckDB.

## ğŸ§© Composants principaux

- **Ingestion** :
  - Lecture des fichiers CSV, JSON, GeoJSON, XLSX
  - Auto-dÃ©tection du sÃ©parateur
  - Encodage explicite (`utf-8`, `ISO-8859-1` fallback)
  - Logging des erreurs

- **Nettoyage** :
  - Standardisation des noms de colonnes
  - Normalisation des types (`float`, `str`, `datetime`)
  - Suppression des lignes et colonnes vides

- **Stockage** :
  - Utilisation de DuckDB pour stocker les datasets nettoyÃ©s
  - Un fichier `.duckdb` unique versionnÃ© localement (pas sur GitHub)

- **Logging** :
  - Journalisation de toutes les Ã©tapes dans `src/log/ingestion_log.txt`
  - Format lisible, horodatÃ©

## ğŸ” ExÃ©cution

La logique mÃ©tier est centralisÃ©e dans `main.py` qui appelle les fonctions de `utils/` Ã  partir des configurations dÃ©finies dans `config/`.

## ğŸ›¡ï¸ Bonnes pratiques respectÃ©es

- SÃ©paration claire entre code mÃ©tier, configuration et stockage
- Utilisation de `.gitignore` pour exclure les fichiers lourds et temporaires
- Environnement virtuel isolÃ© (`venv/`)
- Modularisation des fonctions critiques
